---
title: "hw3: machine teaching for OLS"
author: "Ruixuan Tu (ruixuan@cs.wisc.edu)"
date: "23 February 2023"
---

# Setup

Load packages and set working directory:

```{r results='hide', message=FALSE, warning=FALSE}
knitr::opts_knit$set(root.dir = "/Users/turx/Projects/machine-teaching-23sp/hw03-machine-teaching-ols")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(readxl)
library(tidyverse)
library(lubridate)
library(ggplot2)
set.seed(27021)
```

Functions adapted from homework 02:

```{r}
gen_datasets <- function(n_datasets, len_dataset, a, b, sigma_sq, fixed) {
  dfs <- list(1:n_datasets)
  for (i in 1:n_datasets) {
    dfs[[i]] <- gen_dataset(len_dataset, a, b, sigma_sq, fixed)
  }
  return(dfs)
}

ols_regression_single <- function(x, y) {
  model <- lm(y ~ x)
  return(list(m = model$coefficients[2], b = model$coefficients[1]))
}

ols_regression_batch <- function(ds) {
  ols_results <- list(1:100)
  for (i in 1:100) {
    ols_results[[i]] <- ols_regression_single(ds[[i]]$x, ds[[i]]$y)
  }
  ols_results_df <- tibble(
    m = map_dbl(ols_results, "m"),
    b = map_dbl(ols_results, "b")
  )
  return(ols_results_df)
}
```

# Q1: Synthetic Teacher

## Formulas

$$
\begin{aligned}
\hat{\alpha}-\alpha &= \bar{\varepsilon}-(\hat{\beta}-\beta)\bar{X} \\
\hat{\beta}-\beta &= \frac{\sum_{i=1}^{n} \varepsilon_i (X_i - \bar{X})}{n \hat{\sigma_X^2}}
\end{aligned}
$$

(from https://medium.com/analytics-vidhya/expectation-variance-of-ols-estimates-9acd2b48a635)

## Dataset Generators

```{r}
gen_dataset <- function(n, a, b, sigma_sq, fixed) {
  x <- runif(n, -1, 1)
  eps <- rnorm(n, mean = 0, sd = sigma_sq)
  if (fixed) {
    # fix the regression line to be the truth line
    eps_model <- ols_regression_single(x, eps)
    delta_a <- eps_model$m  # defined above
    delta_b <- eps_model$b  # defined above
    # eliminate the effect of eps
    y <- (a - delta_a) * x + b - delta_b + eps
  } else {
    y <- a * x + b + eps
  }
  return(tibble(x = x, y = y))
}

gen_dataset_shifted <- function(n, a, b, sigma_sq, fixed) {
  while (TRUE) {
    ds <- gen_dataset(n, a, b, sigma_sq, fixed)
    n_pts <- ds %>% nrow()
    for (i in 1:n_pts) {  # select the first point
      for (j in (i + 1):n_pts) {  # select the second point
        # get the regression line of the two points
        x_2pts <- c(ds$x[i], ds$x[j])
        y_2pts <- c(ds$y[i], ds$y[j])
        reg <- ols_regression_single(x_2pts, y_2pts)
        if (reg$m != a || reg$b != b) {
          # at least one pair is not on the y = ax + b line
          return(ds)
        }
      }
    }
  }
}
```

## Experiment Function `run_q1()`

```{r}
run_q1 <- function(n) {
  d <- gen_dataset(n, a = 2, b = -3, sigma_sq = 1, fixed = TRUE)
  model <- ols_regression_single(d$x, d$y)
  plot <- ggplot(data = d) +
    geom_point(aes(x = x, y = y, color = "data points")) +
    geom_abline(aes(intercept = -3, slope = 2, color = "truth")) +
    geom_abline(aes(intercept = model$b, slope = model$m, color = "estimate")) +
    xlim(-5, 5) +
    ylim(-5, 5) +
    ggtitle(bquote("Synthetic Teacher: exact linear regression with n =" ~ .(n)))
  print(plot)
  cat("Estimated slope:", model$m, "\n")
  cat("Estimated intercept:", model$b, "\n")
  cat("True slope:", 2, "\n")
  cat("True intercept:", -3, "\n")
}
```

## Dataset $D_1$

```{r}
run_q1(2)
```

## Dataset $D_2$

```{r}
run_q1(3)
```

# Q2: Pool-based Teacher

## Read Data

```{r}
pool <- read.table("hw3pool.txt", col.names = c("x", "y"))
pool
```

## Algorithm Implementation

```{r}
# define closeness
closeness <- function(m, b, mhat, bhat) {
  (mhat - m)^2 + (bhat - b)^2
}

# generate the set of all multi-subsets of size m from a set of size n given by index 1:n
# an equivalent problem: generate the set of all vectors of n non-negative integers with sum m
gen_vectors <- function(n, m) {
  if (n == 1) {
    return(list(c(m)))
  } else {
    v <- list()
    for (i in 0:m) {
      v <- c(v, lapply(gen_vectors(n - 1, m - i), function(x) c(i, x)))
    }
    return(v)
  }
}

# select the best multi-subset of size n from the dataset df with variables x and y
select_data <- function(df, n) {
  # generate all multi-subsets of size n
  vectors <- gen_vectors(nrow(df), n)
  # generate the multi-subsets
  # an element v_i in vector v means the number of times the i-th point is selected
  multisets <- map(vectors, function(v) {
    multiset <- list(x = c(), y = c())
    for (i in seq_along(v)) {
      multiset$x <- c(multiset$x, rep(df$x[i], v[i]))
      multiset$y <- c(multiset$y, rep(df$y[i], v[i]))
    }
    return(multiset)
  })
  # calculate the closeness of each multi-subset
  closenesses <- map_dbl(multisets, function(s) {
    reg <- ols_regression_single(s$x, s$y)
    return(closeness(2, -3, reg$m, reg$b))
  })
  # return the best vector and its closeness
  min_closeness <- min(closenesses, na.rm = TRUE)
  if (is.na(min_closeness)) {
    return(list(
      vector = NULL,
      multiset = NULL,
      closeness = NA
    ))
  }
  best_idx <- which(closenesses == min_closeness)
  return(list(
    vector = vectors[best_idx],
    multiset = multisets[best_idx],
    closeness = min_closeness
  ))
}
```

## Part 1

```{r}
model <- ols_regression_single(pool$x, pool$y)
plot <- ggplot(data = pool) +
  geom_point(aes(x = x, y = y, color = "data points")) +
  geom_abline(aes(intercept = model$b, slope = model$m, color = "estimate")) +
  geom_abline(aes(intercept = -3, slope = 2, color = "truth")) +
  xlim(-5, 5) +
  ylim(-5, 5) +
  ggtitle(bquote("Pool-based Teacher: linear regression on whole" ~ .(nrow(pool)) ~ "points"))
print(plot)
cat("Estimated slope:", model$m, "\n")
cat("Estimated intercept:", model$b, "\n")
cat("True slope:", 2, "\n")
cat("True intercept:", -3, "\n")
```

## Experiment Function `run_q2()`

```{r}
run_q2 <- function(n) {
  start_time <- Sys.time()
  d <- select_data(pool, n)
  cat("Best dataset (use the first one):\n")
  print(d)
  df_selected <- tibble(
    x = d$multiset[[1]]$x,
    y = d$multiset[[1]]$y
  )
  df_non_selected <- pool %>% filter(!(x %in% df_selected$x & y %in% df_selected$y))
  model <- ols_regression_single(d$multiset[[1]]$x, d$multiset[[1]]$y)
  cat("\nModel of linear regression on the best dataset:\n")
  print(model)
  plot <- ggplot() +
    geom_point(data = df_selected, aes(x = x, y = y, color = "selected points")) +
    geom_point(data = df_non_selected, aes(x = x, y = y, color = "non-selected points")) +
    geom_abline(aes(intercept = -3, slope = 2, color = "truth")) +
    geom_abline(aes(intercept = model$b, slope = model$m, color = "estimate")) +
    xlim(-5, 5) +
    ylim(-5, 5) +
    ggtitle(bquote("Pool-based Teacher: linear regression on best" ~ .(n) ~ "points"))
  print(plot)
  end_time <- Sys.time()
  cat("Time elapsed (second):", end_time - start_time, "\n")
}
```

## Part 2

```{r}
run_q2(2)
```

## Part 3

```{r}
run_q2(3)
```
